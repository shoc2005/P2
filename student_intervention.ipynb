{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?\n",
    "\n",
    "This is classification problem. We neede to classify students into two groups: needed or not needed early intervention (two-class classification), hence classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n",
    "_To execute a code cell, click inside it and press **Shift+Enter**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "#Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 31\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "n_students = student_data.shape[0]\n",
    "n_features = student_data.shape[1]\n",
    "n_passed = pd.value_counts(student_data.passed == \"yes\")[True]\n",
    "n_failed = pd.value_counts(student_data.passed == \"yes\")[False]\n",
    "grad_rate = float(n_passed)/(n_passed + n_failed)*100\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "global X_all\n",
    "global y_all\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = pd.DataFrame(student_data[target_col])  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "    #print outX.columns[:]\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "global X_all\n",
    "global y_all\n",
    "X_all = preprocess_features(X_all)\n",
    "y_all = preprocess_features(y_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "\n",
    "train_set_X = []\n",
    "train_set_y = []\n",
    "test_set_X =[]\n",
    "test_set_y =[]\n",
    "\n",
    "rs = ShuffleSplit(num_all,  n_iter=1, train_size=num_train, test_size=num_test)\n",
    "\n",
    "\n",
    "def next_batch(rs, train_size, test_size, keep_test_set_constant=False):\n",
    "    # get training and testing set for different size and constant or not testing set\n",
    "    for train, test in rs:\n",
    "        test_set = test\n",
    "        train_set = train\n",
    "    \n",
    "    X_ = X_all.values\n",
    "    y_ = y_all.values\n",
    "    \n",
    "        \n",
    "    X_train = np.zeros((train_size, 48))\n",
    "    y_train = np.zeros((train_size, 1))\n",
    "    X_test = np.zeros((test_size, 48))\n",
    "    y_test = np.zeros((test_size, 1))\n",
    "    train_set = shuffle(train_set)\n",
    "    train_set = train_set[0:train_size]\n",
    "    for i, item in enumerate(train_set):\n",
    "        X_train[i][:] = X_[item][:]\n",
    "        y_train[i][:] = y_[item][:]\n",
    "   \n",
    "    if not keep_test_set_constant:\n",
    "        test_set = shuffle(test_set)\n",
    "    test_set = test_set[0:test_size]    \n",
    "    for i, item in enumerate(test_set):\n",
    "        X_test[i][:] = X_[item][:]\n",
    "        y_test[i][:] = y_[item][:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "X_train, y_train, X_test, y_test = next_batch(rs, train_size = num_train ,test_size = num_test, keep_test_set_constant=True)\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What is the theoretical O(n) time & space complexity in terms of input size?\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "    return \"{:.5f}\".format(end - start)\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "\n",
    "def create_classifier(type_of = \"Tree\", weights = None):\n",
    "    if type_of == \"Tree\":\n",
    "        return tree.DecisionTreeClassifier(class_weight=weights) #used parameter class_weight because dataset not balanced\n",
    "    elif type_of == \"SVM\":\n",
    "        return svm.SVC(class_weight=weights) #used parameter class_weight because dataset not balanced\n",
    "    elif type_of == \"GrBoost\":\n",
    "        return GradientBoostingClassifier(n_estimators=100, learning_rate=.1, max_depth=3)\n",
    "    else:\n",
    "        raise ValueError(\"Classifier not found\", type_of)\n",
    "        \n",
    "\n",
    "clf = create_classifier(\"Tree\")\n",
    "train_classifier(clf, X_train, y_train)\n",
    "print clf  # you can inspect the learned model by printing it\n",
    "#print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00048\n",
      "F1 score for training set: 1.000 in 0.00048 sec\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nPrediction time (secs): {:.5f}\".format(end - start)\n",
    "    return f1_score(target, y_pred), \"{:.5f}\".format(end - start)\n",
    "\n",
    "train_f1_score, time_ = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {:.3f} in {} sec\".format(train_f1_score, time_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00043\n",
      "F1 score for test set: (0.7086614173228345, '0.00043')\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "$$\\newline$$**Complexity**\n",
    "\n",
    "Decision Trees in general need: $$O(n_{samples} \\times n_{features} \\times \\log n_{samples}).$$ to construct balanced binary tree and query time: $$O(\\log n_{samples}).$$ For the project`s problem complexity is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity for construction O(31890) for 100 samples\n",
      "Complexity for query O(6) for 100 samples\n"
     ]
    }
   ],
   "source": [
    "samples_ = 100\n",
    "print \"Complexity for construction O({:d}) for {:d} samples\".format(int(samples_*48*np.log2(samples_)), samples_)\n",
    "print \"Complexity for query O({:d}) for {:d} samples\".format(int(np.log2(samples_)), samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newline$$**General applications**\n",
    "\n",
    "Decision-trees best suited to problems with following characteristics:\n",
    "\n",
    "- Samples are represented by attribute-value pairs. Each feature takes on a small number of disjoint possible values (e.g., man, woman).\n",
    "- Best for two possible output values.\n",
    "- Problems with disjunctive descriptions.\n",
    "- The training data may contain errors. Decision-tree learning methods are robust for errors.\n",
    "- Training data may contain missing attribute data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strengths and weaknesses \n",
    "$$\\newline$$_Strengths_:\n",
    "\n",
    "- Simple to understand and to interpret, especially using graphic representation.\n",
    "- White box - learned model will be explained using boolean logic.\n",
    "- Data preparation not very hard. No needs data normalization, dummy and empty value filtering.\n",
    "- Computation cost is relatively low - logarithmic for tree training and prediction.\n",
    "- Possible validate model using statistical tests.\n",
    "\n",
    "$$\\newline$$_Weaknesses_:\n",
    "\n",
    "- Decision tree learners create biased trees if some classes dominate. Need balanced training data for every class or need equal number of samples for each class.\n",
    "- Usually decision-tree learners generate overfitted models and for preventing that depth max and minimum number of samples at a leaf node are necessary.\n",
    "- Small variations of data might result of completely different trees being generated.\n",
    "- Learning optimal decision-tree is NP-problem. In practice heuristic methods usually used and that are not guarantee globally optimal tree (to avoid this multiple decision trees will be used with randomly sampled features and samples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Student feature dataset contains NaN or empty values ? False\n",
      "Is Student label dataset contains NaN or empty values ? False\n",
      "Weights of data balance:\n",
      "Training data: {0.0: 0.31333333333333335, 1.0: 0.6866666666666666}\n",
      "Testing data: {0.0: 0.37894736842105264, 1.0: 0.6210526315789474}\n"
     ]
    }
   ],
   "source": [
    "def check_of_empty_values(data_set): \n",
    "    #create empty Data frame for broken data (NaN or empty)\n",
    "    return data_set.isnull().values.any()\n",
    "\n",
    "\n",
    "def get_labels_balance(data_set):\n",
    "    #coutn labels in training or testing set and return label weights\n",
    "    unique, counts = np.unique(data_set, return_counts=True)\n",
    "    label_weights = {}\n",
    "    for i in range(0, unique.shape[0]):\n",
    "        label_weights[unique[i]] = float(counts[i])/data_set.shape[0]\n",
    "    return label_weights\n",
    "\n",
    "\n",
    "print \"Is Student feature dataset contains NaN or empty values ?\", check_of_empty_values(X_all)\n",
    "print \"Is Student label dataset contains NaN or empty values ?\", check_of_empty_values(y_all)\n",
    "print \"Weights of data balance:\"\n",
    "print \"Training data:\", get_labels_balance(y_train)\n",
    "print \"Testing data:\", get_labels_balance(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justification\n",
    "$$\\newline$$**Decision-Trees will be used for classification and regression problems with single and multi-variable output.\n",
    "Since DT is white box it is possible to explain prediction results and it may be useful for stuff.**\n",
    "\n",
    "**Looking on the students data the DT also may be used for such problem because:**\n",
    "\n",
    "- most of attributes values are two-pared;\n",
    "- predicting value belongs for the two classes;\n",
    "- dataset specially was not prepared (excluding data type convertion to numbers and using dummies variables).\n",
    " \n",
    "Although data set is not balanced this may mitigated by using label_weights parameter for classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.001\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00014\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00012\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00015\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00012\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.004\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00020\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00013\n",
      "\n",
      "Decision Tree training results\n",
      "   F1_testing  F1_training  Size Time test Time train\n",
      "1    0.744186            1   100   0.00012    0.00114\n",
      "2    0.746269            1   200   0.00012    0.00231\n",
      "3    0.692913            1   300   0.00013    0.00439\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    #print \"------------------------------------------\"\n",
    "    #print \"Training set size: {}\".format(len(X_train))\n",
    "    tr_weights = get_labels_balance(y_train)\n",
    "    time_str = train_classifier(clf, X_train, y_train)\n",
    "    tr_pred = predict_labels(clf, X_train, y_train)\n",
    "    #print \"F1 score for training set: {}\".format(tr_pred)\n",
    "    ts_pred = predict_labels(clf, X_test, y_test)\n",
    "    #print \"F1 score for test set: {}\".format(ts_pred)\n",
    "    return [{\"Size\":len(X_train), \"Time\":time_str, \"F1_training\":tr_pred, \"F1_testing\":ts_pred}]\n",
    "\n",
    "#Create Pandas table\n",
    "def append_value(frame, dict_values, i):\n",
    "    if type(frame) is pd.DataFrame:\n",
    "        new_frame = pd.DataFrame(dict_values, index=[i])\n",
    "        return pd.concat([frame, new_frame])\n",
    "    else:\n",
    "        return pd.DataFrame(dict_values, index=[i])\n",
    "        \n",
    "\n",
    "# TODO: Run the helper function above for desired subsets of training data\n",
    "# Note: Keep the test set constant\n",
    "training_set_sizes = [100, 200, 300]\n",
    "tree_table = None\n",
    "i = 1\n",
    "for set_size in training_set_sizes:\n",
    "    X_train, y_train, X_test, y_test = next_batch(rs, train_size=set_size, test_size=num_test, keep_test_set_constant=True)\n",
    "    # Create Decision-tree classifier and use label weights\n",
    "    lab_weights = get_labels_balance(y_train)\n",
    "    clf = create_classifier(\"Tree\", weights=lab_weights)\n",
    "    train_time = train_classifier(clf, X_train, y_train)\n",
    "    f1_training, tr_time = predict_labels(clf, X_train, y_train)\n",
    "    f1_testing, ts_time = predict_labels(clf, X_test, y_test)\n",
    "    row = {\"Size\":set_size, \"F1_training\":f1_training, \"F1_testing\":f1_testing, \"Time train\":train_time, \"Time test\":ts_time}\n",
    "    tree_table = append_value(tree_table, row, i)\n",
    "    i += 1\n",
    "\n",
    "# Result of Decision Tree\n",
    "print \"\\nDecision Tree training results\\n\", tree_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "$$\\newline$$Training time increase with approximately same rate as training size increase. Testing time fluctuating on the same level.\n",
    "Model for all training sets looks like overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "#### Complexity.\n",
    "\n",
    "The core of SVM is quadratic programming problem (QP), separating support vectors from the rest of the training data.\n",
    "Support Vector Machines for scipy implementation needs between $$O(n_{features} \\times n^{2}_{samples})$$ and $$O(n_{features} \\times n^{3}_{samples})$$\n",
    "For the project's problem complexity is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity for construction O(4.800e+05) for 100 samples\n",
      "Complexity for query O(4.800e+07) for 100 samples\n"
     ]
    }
   ],
   "source": [
    "samples_ = 100\n",
    "print \"Complexity for construction O({:.3e}) for {:d} samples\".format(int(48*samples_**2), samples_)\n",
    "print \"Complexity for query O({:.3e}) for {:d} samples\".format(int(48*samples_**3), samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General applications.\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "Effective in high dimensional spaces.\n",
    "Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "\n",
    "#### Strengths and weaknesses\n",
    "$$\\newline$$_Strengths_:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "$$\\newline$$_Weakness_:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, the method is likely to give poor performances.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "\n",
    "\n",
    "#### Justification\n",
    "\n",
    "- Number of features is relatively large compare to number of samples.\n",
    "- SVM be used for two-class problem.\n",
    "- Prediction will use relatively small piece of memory and be quick- only support vectors are stored.\n",
    "- SVM also will be used for classification problems.\n",
    "- Kernel function can express domain knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.057\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.00156\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.00138\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.007\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.00458\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.00225\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.015\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.01043\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.00351\n",
      "\n",
      "SVM training results\n",
      "   F1_testing  F1_training  Size Time test Time train\n",
      "1    0.841463     0.765432   100   0.00138    0.05680\n",
      "2    0.782051     0.807339   200   0.00225    0.00652\n",
      "3    0.789809     0.807157   300   0.00351    0.01497\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "training_set_sizes = [100, 200, 300]\n",
    "svm_table = None\n",
    "i = 1\n",
    "for set_size in training_set_sizes:\n",
    "    X_train, y_train, X_test, y_test = next_batch(rs, train_size=set_size, test_size=num_test, keep_test_set_constant=True)\n",
    "    # Create Decision-tree classifier and use label weights\n",
    "    lab_weights = get_labels_balance(y_train)\n",
    "    #print lab_weights\n",
    "    clf = create_classifier(\"SVM\", weights=lab_weights)\n",
    "    y_train = y_train.reshape((y_train.shape[0]))\n",
    "\n",
    "    train_time = train_classifier(clf, X_train, y_train)\n",
    "    f1_training, tr_time = predict_labels(clf, X_train, y_train)\n",
    "    f1_testing, ts_time = predict_labels(clf, X_test, y_test)\n",
    "    row = {\"Size\":set_size, \"F1_training\":f1_training, \"F1_testing\":f1_testing, \"Time train\":train_time, \"Time test\": ts_time}\n",
    "    svm_table = append_value(svm_table, row, i)\n",
    "    i += 1\n",
    "\n",
    "# SVM training results\n",
    "print \"\\nSVM training results\\n\", svm_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary.\n",
    "The training time very rapidly increase with increasing training set. For 100 is 0.0023, but for 300 is 0.0143 or: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 times\n"
     ]
    }
   ],
   "source": [
    "print int(0.0143/0.0023), \"times\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing time increase with approximately same rate as set size increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "#### Complexity.\n",
    "\n",
    "The algorithm for Boosting Trees evolved from the application of boosting methods to regression trees. The general idea is to compute a sequence of (very) simple trees, where each successive tree is built for the prediction residuals of the preceding tree. The complexity of Gradient Boosting depends on number of decision trees and their depth and features number.\n",
    "\n",
    "#### General applications.\n",
    "\n",
    "Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems.\n",
    "\n",
    "#### Strengths and weakness\n",
    "\n",
    "$$\\newline$$*Strengths*:\n",
    "\n",
    "- Natural handling of data of mixed type (= heterogeneous features)\n",
    "- Predictive power\n",
    "- Robustness to outliers in output space (via robust loss functions)\n",
    "- Robustness to data scaling\n",
    "\n",
    "$$\\newline$$*Weakness*:\n",
    "\n",
    "- Scalability, due to the sequential nature of boosting it can hardly be parallelized.\n",
    "\n",
    "#### Justifications\n",
    "\n",
    "- GradientBoostingClassifier supports both binary and multi-label classification\n",
    "- Prediction time relatively low\n",
    "- Supports mixed type of features and not needed data normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoostingClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.126\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00073\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00066\n",
      "Training GradientBoostingClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.169\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00105\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00062\n",
      "Training GradientBoostingClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.197\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00150\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.00070\n",
      "\n",
      "Gradient Boosting training results\n",
      "   F1_testing  F1_training  Size Time test Time train\n",
      "1    0.746269     1.000000   100   0.00066    0.12638\n",
      "2    0.761194     0.992647   200   0.00062    0.16908\n",
      "3    0.840000     0.970297   300   0.00070    0.19736\n"
     ]
    }
   ],
   "source": [
    "training_set_sizes = [100, 200, 300]\n",
    "boost_table = None\n",
    "i = 1\n",
    "for set_size in training_set_sizes:\n",
    "    X_train, y_train, X_test, y_test = next_batch(rs, train_size=set_size, test_size=num_test, keep_test_set_constant=True)\n",
    "\n",
    "    clf = create_classifier(\"GrBoost\", weights=lab_weights)\n",
    "    y_train = y_train.reshape((y_train.shape[0]))\n",
    "\n",
    "    train_time = train_classifier(clf, X_train, y_train)\n",
    "    f1_training, tr_time = predict_labels(clf, X_train, y_train)\n",
    "    f1_testing, ts_time = predict_labels(clf, X_test, y_test)\n",
    "    row = {\"Size\":set_size, \"F1_training\":f1_training, \"F1_testing\":f1_testing, \"Time train\":train_time, \"Time test\":ts_time}\n",
    "    boost_table = append_value(boost_table, row, i)\n",
    "    i += 1\n",
    "\n",
    "# Gradient Boosting training results\n",
    "print \"\\nGradient Boosting training results\\n\", boost_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Training time slowly increase as increasing training set size. Prediction time fluctuates on same level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of algoritms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "   F1_testing  F1_training  Size Time test Time train\n",
      "1    0.744186            1   100   0.00012    0.00114\n",
      "2    0.746269            1   200   0.00012    0.00231\n",
      "3    0.692913            1   300   0.00013    0.00439\n",
      "\n",
      "SVM:\n",
      "   F1_testing  F1_training  Size Time test Time train\n",
      "1    0.841463     0.765432   100   0.00138    0.05680\n",
      "2    0.782051     0.807339   200   0.00225    0.00652\n",
      "3    0.789809     0.807157   300   0.00351    0.01497\n",
      "\n",
      "Gradient boosting:\n",
      "   F1_testing  F1_training  Size Time test Time train\n",
      "1    0.746269     1.000000   100   0.00066    0.12638\n",
      "2    0.761194     0.992647   200   0.00062    0.16908\n",
      "3    0.840000     0.970297   300   0.00070    0.19736\n"
     ]
    }
   ],
   "source": [
    "print \"Decision Tree:\"\n",
    "print tree_table\n",
    "\n",
    "print \"\\nSVM:\"\n",
    "print svm_table\n",
    "\n",
    "print \"\\nGradient boosting:\"\n",
    "print boost_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
